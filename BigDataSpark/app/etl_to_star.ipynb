{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a962eb-fb2a-4fee-9fcf-6e14abe9ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as _sum, year, month, dayofmonth, quarter, row_number, desc, first, corr, count, lit, concat_ws, avg\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ba0a0",
   "metadata": {},
   "source": [
    "# Работа с ETL процессами\n",
    "\n",
    "## 1. Подключение с помощью PySpark и проверка данных из `mock_data`\n",
    "\n",
    "Нужно преобразовать данные из Postgres `mock_data` в `снежинку`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1c11ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/18 13:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/18 13:41:40 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=1, customer_first_name='Barron', customer_last_name='Rawlyns', customer_age=61, customer_email='bmassingham0@army.mil', customer_country='China', customer_postal_code=None, customer_pet_type='cat', customer_pet_name='Priscella', customer_pet_breed='Labrador Retriever', seller_first_name='Bevan', seller_last_name='Massingham', seller_email='bmassingham0@answers.com', seller_country='Indonesia', seller_postal_code=None, product_name='Dog Food', product_category='Food', product_price=Decimal('77.97'), product_quantity=89, sale_date=datetime.date(2021, 5, 14), sale_customer_id=1, sale_seller_id=1, sale_product_id=1, sale_quantity=4, sale_total_price=Decimal('487.70'), store_name='Youopia', store_location='Suite 75', store_city='Xichehe', store_state=None, store_country='United States', store_phone='564-244-8660', store_email='bmassingham0@networkadvertising.org', pet_category='Cats', product_weight=Decimal('13.40'), product_color='Indigo', product_size='Medium', product_brand='Skajo', product_material='Steel', product_description='Aliquam quis turpis eget elit sodales scelerisque. Mauris sit amet eros. Suspendisse accumsan tortor quis turpis.\\n\\nSed ante. Vivamus tortor. Duis mattis egestas metus.', product_rating=Decimal('2.1'), product_reviews=97, product_release_date=datetime.date(2011, 10, 19), product_expiry_date=datetime.date(2028, 10, 21), supplier_name='Tagcat', supplier_contact='Bevan Massingham', supplier_email='bmassingham0@unblog.fr', supplier_phone='914-877-7062', supplier_address='Suite 25', supplier_city='Kletek', supplier_country='China')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Инициализация SparkSession с драйвером PostgreSQL\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"ETL to Star\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Чтение данных из PostgreSQL\n",
    "pg_url = \"jdbc:postgresql://postgres:5432/bober_db\"\n",
    "pg_properties = {\"user\": \"bober\", \"password\": \"bober\", \"driver\": \"org.postgresql.Driver\"}\n",
    "df = spark.read.jdbc(url=pg_url, table=\"mock_data\", properties=pg_properties)\n",
    "\n",
    "# Проверка чтения данных\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b54206a",
   "metadata": {},
   "source": [
    "## 2. Создаем модель данных снежинку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7cff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 1. dim_date (surrogate key — date_id)\n",
    "# ===================================================================\n",
    "dim_date = df.select(col(\"sale_date\").alias(\"full_date\")) \\\n",
    "    .distinct() \\\n",
    "    .filter(col(\"full_date\").isNotNull()) \\\n",
    "    .withColumn(\"date_id\", row_number().over(Window.orderBy(\"full_date\"))) \\\n",
    "    .withColumn(\"year\", year(\"full_date\")) \\\n",
    "    .withColumn(\"month\", month(\"full_date\")) \\\n",
    "    .withColumn(\"day\", dayofmonth(\"full_date\")) \\\n",
    "    .withColumn(\"quarter\", quarter(\"full_date\"))\n",
    "\n",
    "dim_date.write.jdbc(url=pg_url, table=\"dim_date\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 2. dim_customer (natural key — sale_customer_id, предполагаем уникальность)\n",
    "# ===================================================================\n",
    "dim_customer = df.select(\n",
    "    col(\"sale_customer_id\").alias(\"customer_id\"),\n",
    "    col(\"customer_first_name\").alias(\"first_name\"),\n",
    "    col(\"customer_last_name\").alias(\"last_name\"),\n",
    "    col(\"customer_age\").alias(\"age\"),\n",
    "    col(\"customer_email\").alias(\"email\"),\n",
    "    col(\"customer_country\").alias(\"country\"),\n",
    "    col(\"customer_postal_code\").alias(\"postal_code\")\n",
    ").distinct()\n",
    "\n",
    "dim_customer.write.jdbc(url=pg_url, table=\"dim_customer\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 3. dim_seller (natural key — sale_seller_id)\n",
    "# ===================================================================\n",
    "dim_seller = df.select(\n",
    "    col(\"sale_seller_id\").alias(\"seller_id\"),\n",
    "    col(\"seller_first_name\").alias(\"first_name\"),\n",
    "    col(\"seller_last_name\").alias(\"last_name\"),\n",
    "    col(\"seller_email\").alias(\"email\"),\n",
    "    col(\"seller_country\").alias(\"country\"),\n",
    "    col(\"seller_postal_code\").alias(\"postal_code\")\n",
    ").distinct()\n",
    "\n",
    "dim_seller.write.jdbc(url=pg_url, table=\"dim_seller\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 4. dim_product (natural key — sale_product_id)\n",
    "# ===================================================================\n",
    "dim_product = df.select(\n",
    "    col(\"sale_product_id\").alias(\"product_id\"),\n",
    "    col(\"product_name\").alias(\"name\"),\n",
    "    col(\"product_category\").alias(\"category\"),          # или pet_category — выбирай то, что подходит\n",
    "    col(\"product_price\").alias(\"price\"),\n",
    "    col(\"product_weight\").alias(\"weight\"),\n",
    "    col(\"product_color\").alias(\"color\"),\n",
    "    col(\"product_size\").alias(\"size\"),\n",
    "    col(\"product_brand\").alias(\"brand\"),\n",
    "    col(\"product_material\").alias(\"material\"),\n",
    "    col(\"product_description\").alias(\"description\"),\n",
    "    col(\"product_rating\").alias(\"rating\"),\n",
    "    col(\"product_reviews\").alias(\"reviews\"),\n",
    "    col(\"product_release_date\").alias(\"release_date\"),\n",
    "    col(\"product_expiry_date\").alias(\"expiry_date\")\n",
    ").distinct()\n",
    "\n",
    "dim_product.write.jdbc(url=pg_url, table=\"dim_product\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 5. dim_store (surrogate key)\n",
    "# ===================================================================\n",
    "dim_store_raw = df.select(\n",
    "    \"store_name\", \"store_location\", \"store_city\",\n",
    "    \"store_state\", \"store_country\", \"store_phone\", \"store_email\"\n",
    ").distinct()\n",
    "\n",
    "store_window = Window.orderBy(\"store_name\", \"store_city\", \"store_country\")\n",
    "dim_store = dim_store_raw.withColumn(\"store_id\", row_number().over(store_window))\n",
    "dim_store.write.jdbc(url=pg_url, table=\"dim_store\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 6. dim_supplier (surrogate key)\n",
    "# ===================================================================\n",
    "supplier_window = Window.orderBy(\"supplier_name\", \"supplier_city\", \"supplier_country\")\n",
    "dim_supplier = df.select(\n",
    "    \"supplier_name\",\n",
    "    col(\"supplier_contact\").alias(\"contact\"),\n",
    "    \"supplier_email\",\n",
    "    \"supplier_phone\",\n",
    "    \"supplier_address\",\n",
    "    \"supplier_city\",\n",
    "    \"supplier_country\"\n",
    ").distinct() \\\n",
    "    .withColumn(\"supplier_id\", row_number().over(supplier_window))\n",
    "\n",
    "dim_supplier.write.jdbc(url=pg_url, table=\"dim_supplier\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 7. dim_pet (surrogate key, привязка к клиенту)\n",
    "# ===================================================================\n",
    "dim_pet_raw = df.select(\n",
    "    col(\"sale_customer_id\").alias(\"customer_id\"),\n",
    "    col(\"customer_pet_type\").alias(\"pet_type\"),\n",
    "    col(\"customer_pet_name\").alias(\"pet_name\"),\n",
    "    col(\"customer_pet_breed\").alias(\"pet_breed\"),\n",
    "    col(\"pet_category\").alias(\"pet_category\")        # или просто \"category\"\n",
    ").distinct()\n",
    "\n",
    "# Окно определяем ПОСЛЕ select + alias, чтобы использовать новые имена колонок\n",
    "pet_window = Window.orderBy(\"customer_id\", \"pet_name\", \"pet_type\")\n",
    "dim_pet = dim_pet_raw.withColumn(\"pet_id\", row_number().over(pet_window))\n",
    "dim_pet.write.jdbc(url=pg_url, table=\"dim_pet\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "# ===================================================================\n",
    "# 8. fact_sales — собираем всё вместе\n",
    "# ===================================================================\n",
    "fact_sales = df \\\n",
    "    .join(dim_date, df.sale_date == dim_date.full_date, \"left\") \\\n",
    "    .join(dim_store, \n",
    "          (df.store_name == dim_store.store_name) &\n",
    "          (df.store_location == dim_store.store_location) &\n",
    "          (df.store_city == dim_store.store_city) &\n",
    "          (df.store_state == dim_store.store_state) &\n",
    "          (df.store_country == dim_store.store_country) &\n",
    "          (df.store_phone == dim_store.store_phone) &\n",
    "          (df.store_email == dim_store.store_email), \"left\") \\\n",
    "    .join(dim_supplier,\n",
    "          (df.supplier_name == dim_supplier.supplier_name) &\n",
    "          (df.supplier_city == dim_supplier.supplier_city) &\n",
    "          (df.supplier_country == dim_supplier.supplier_country), \"left\") \\\n",
    "    .join(dim_pet,\n",
    "          (df.sale_customer_id == dim_pet.customer_id) &\n",
    "          (df.customer_pet_name == dim_pet.pet_name) &\n",
    "          (df.customer_pet_type == dim_pet.pet_type), \"left\") \\\n",
    "    .select(\n",
    "        col(\"id\").alias(\"sale_id\"),\n",
    "        col(\"sale_customer_id\").alias(\"customer_id\"),\n",
    "        col(\"pet_id\"),\n",
    "        col(\"sale_seller_id\").alias(\"seller_id\"),\n",
    "        col(\"sale_product_id\").alias(\"product_id\"),\n",
    "        col(\"store_id\"),\n",
    "        col(\"supplier_id\"),\n",
    "        col(\"date_id\"),\n",
    "        col(\"sale_quantity\").alias(\"sale_quantity\"),\n",
    "        col(\"sale_total_price\").alias(\"sale_total_price\")\n",
    "    )\n",
    "\n",
    "fact_sales.write.jdbc(url=pg_url, table=\"fact_sales\", mode=\"overwrite\", properties=pg_properties)\n",
    "\n",
    "print(\"Звёздная схема успешно построена! Проверить можно в DBeaver: SELECT * FROM fact_sales LIMIT 5;\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aefaabe",
   "metadata": {},
   "source": [
    "## 3. Создание витрин в clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e82cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/18 13:42:20 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Инициализация SparkSession с драйвером ClickHouse\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"spark://spark-master:7077\") \\\n",
    "#     .appName(\"ETL to Star\") \\\n",
    "#     .config(\"spark.jars\", \"/opt/spark/jars/clickhouse-jdbc-0.6.0.jar\") \\\n",
    "#     .getOrCreate()\n",
    "    \n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"spark://spark-master:7077\") \\\n",
    "#     .appName(\"Spark_and_ClickHouse\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ClickHouse ETL\") \\\n",
    "    .config(\"spark.jars\",\n",
    "            \"/opt/spark/jars/clickhouse-jdbc-0.6.0.jar,\"\n",
    "            \"/opt/spark/jars/clickhouse-spark-connector_2.12-0.8.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Настройки подключения к БД и Spark\n",
    "ch_url = \"jdbc:clickhouse://clickhouse:8123/default\"\n",
    "ch_options = {\n",
    "    \"host\": \"clickhouse\",\n",
    "    \"port\": \"8123\",\n",
    "    \"user\": \"default\",\n",
    "    \"password\": \"\",\n",
    "    \"database\": \"default\"\n",
    "}\n",
    "# ch_properties = {\n",
    "#     \"driver\": \"com.clickhouse.jdbc.ClickHouseDriver\",\n",
    "#     \"user\": \"default\",\n",
    "#     \"password\": \"\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95553d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем все таблицы звезды\n",
    "fact = spark.read.jdbc(url=pg_url, table=\"fact_sales\", properties=pg_properties)\n",
    "dim_product = spark.read.jdbc(url=pg_url, table=\"dim_product\", properties=pg_properties)\n",
    "dim_customer = spark.read.jdbc(url=pg_url, table=\"dim_customer\", properties=pg_properties)\n",
    "dim_store = spark.read.jdbc(url=pg_url, table=\"dim_store\", properties=pg_properties)\n",
    "dim_supplier = spark.read.jdbc(url=pg_url, table=\"dim_supplier\", properties=pg_properties)\n",
    "dim_date = spark.read.jdbc(url=pg_url, table=\"dim_date\", properties=pg_properties)\n",
    "dim_date.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "569228a5-3051-4a1e-aaa4-8393b1893645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(full_date=datetime.date(2021, 1, 1), date_id=1, year=2021, month=1, day=1, quarter=1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_date.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13f79772-ebb6-47b8-84f0-3d55d3629963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Колонки fact_sales: ['sale_id', 'customer_id', 'pet_id', 'seller_id', 'product_id', 'store_id', 'supplier_id', 'date_id', 'sale_quantity', 'sale_total_price']\n",
      "Колонки dim_product: ['product_id', 'name', 'category', 'price', 'weight', 'color', 'size', 'brand', 'material', 'description', 'rating', 'reviews', 'release_date', 'expiry_date']\n",
      "Колонки dim_customer: ['customer_id', 'first_name', 'last_name', 'age', 'email', 'country', 'postal_code']\n",
      "Колонки dim_store: ['store_name', 'store_location', 'store_city', 'store_state', 'store_country', 'store_phone', 'store_email', 'store_id']\n",
      "Колонки dim_supplier: ['supplier_name', 'contact', 'supplier_email', 'supplier_phone', 'supplier_address', 'supplier_city', 'supplier_country', 'supplier_id']\n",
      "Колонки dim_date: ['full_date', 'date_id', 'year', 'month', 'day', 'quarter']\n"
     ]
    }
   ],
   "source": [
    "# Проверим какие колонки есть в каждой таблице\n",
    "print(\"Колонки fact_sales:\", fact.columns)\n",
    "print(\"Колонки dim_product:\", dim_product.columns)\n",
    "print(\"Колонки dim_customer:\", dim_customer.columns)\n",
    "print(\"Колонки dim_store:\", dim_store.columns)\n",
    "print(\"Колонки dim_supplier:\", dim_supplier.columns)\n",
    "print(\"Колонки dim_date:\", dim_date.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "873f17d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ch_properties' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ===================================================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 1. Витрина продаж по продуктам\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ===================================================================\u001b[39;00m\n\u001b[1;32m      4\u001b[0m product_vitrina \u001b[38;5;241m=\u001b[39m fact\u001b[38;5;241m.\u001b[39mjoin(dim_product, fact\u001b[38;5;241m.\u001b[39mproduct_id \u001b[38;5;241m==\u001b[39m dim_product\u001b[38;5;241m.\u001b[39mproduct_id) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupBy(dim_product\u001b[38;5;241m.\u001b[39mproduct_id, dim_product\u001b[38;5;241m.\u001b[39mname, dim_product\u001b[38;5;241m.\u001b[39mcategory) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39magg(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         first(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviews\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_count\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     )\n\u001b[0;32m---> 12\u001b[0m product_vitrina\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mjdbc(url\u001b[38;5;241m=\u001b[39mch_url, table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvitrina_product_sales\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m, properties\u001b[38;5;241m=\u001b[39m\u001b[43mch_properties\u001b[49m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Топ-10 самых продаваемых (отдельная таблица для удобства проверки)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m top10_products \u001b[38;5;241m=\u001b[39m product_vitrina\u001b[38;5;241m.\u001b[39morderBy(desc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_quantity\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ch_properties' is not defined"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 1. Витрина продаж по продуктам\n",
    "# ===================================================================\n",
    "product_vitrina = fact.join(dim_product, fact.product_id == dim_product.product_id) \\\n",
    "    .groupBy(dim_product.product_id, dim_product.name, dim_product.category) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        first(\"rating\").alias(\"avg_rating\"),\n",
    "        first(\"reviews\").alias(\"review_count\")\n",
    "    )\n",
    "product_vitrina.write.jdbc(url=ch_url, table=\"vitrina_product_sales\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Топ-10 самых продаваемых (отдельная таблица для удобства проверки)\n",
    "top10_products = product_vitrina.orderBy(desc(\"total_quantity\")).limit(10)\n",
    "top10_products.write.jdbc(url=ch_url, table=\"top10_sold_products\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Выручка по категориям (отдельная таблица)\n",
    "category_revenue = product_vitrina.groupBy(\"category\") \\\n",
    "    .agg(_sum(\"total_revenue\").alias(\"category_revenue\"))\n",
    "category_revenue.write.jdbc(url=ch_url, table=\"category_revenue\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 2. Витрина продаж по клиентам\n",
    "# ===================================================================\n",
    "customer_vitrina = fact.join(dim_customer, fact.customer_id == dim_customer.customer_id) \\\n",
    "    .groupBy(dim_customer.customer_id, dim_customer.first_name, dim_customer.last_name, dim_customer.country) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_total_price\").alias(\"total_spent\"),\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        avg(\"sale_total_price\").alias(\"avg_check\")\n",
    "    ) \\\n",
    "    .withColumn(\"customer_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\"))) \\\n",
    "    .select(\"customer_id\", \"customer_name\", \"country\", \"total_spent\", \"order_count\", \"avg_check\")\n",
    "customer_vitrina.write.jdbc(url=ch_url, table=\"vitrina_customer_sales\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Топ-10 клиентов\n",
    "top10_customers = customer_vitrina.orderBy(desc(\"total_spent\")).limit(10)\n",
    "top10_customers.write.jdbc(url=ch_url, table=\"top10_customers_by_spent\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Распределение по странам (отдельная таблица)\n",
    "customer_country_dist = customer_vitrina.groupBy(\"country\") \\\n",
    "    .agg(\n",
    "        _sum(\"total_spent\").alias(\"total_spent_by_country\"),\n",
    "        count(\"*\").alias(\"customer_count\")\n",
    "    )\n",
    "customer_country_dist.write.jdbc(url=ch_url, table=\"customer_country_distribution\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 3. Витрина продаж по времени\n",
    "# ===================================================================\n",
    "time_vitrina = fact.join(dim_date, fact.date_id == dim_date.date_id) \\\n",
    "    .groupBy(dim_date.year, dim_date.month) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        _sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        count(\"*\").alias(\"order_count\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_check\", col(\"total_revenue\") / col(\"order_count\")) \\\n",
    "    .withColumn(\"avg_order_size\", col(\"total_quantity\") / col(\"order_count\"))\n",
    "time_vitrina.write.jdbc(url=ch_url, table=\"vitrina_time_sales\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 4. Витрина продаж по магазинам\n",
    "# ===================================================================\n",
    "store_vitrina = fact.join(dim_store, fact.store_id == dim_store.store_id) \\\n",
    "    .groupBy(dim_store.store_id, dim_store.store_name, dim_store.city, dim_store.country) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        avg(\"sale_total_price\").alias(\"avg_check\")\n",
    "    )\n",
    "store_vitrina.write.jdbc(url=ch_url, table=\"vitrina_store_sales\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Топ-5 магазинов\n",
    "top5_stores = store_vitrina.orderBy(desc(\"total_revenue\")).limit(5)\n",
    "top5_stores.write.jdbc(url=ch_url, table=\"top5_stores_by_revenue\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 5. Витрина продаж по поставщикам\n",
    "# ===================================================================\n",
    "supplier_vitrina = fact.join(dim_product[[\"product_id\", \"price\"]], fact.product_id == dim_product.product_id) \\\n",
    "    .join(dim_supplier, fact.supplier_id == dim_supplier.supplier_id) \\\n",
    "    .groupBy(dim_supplier.supplier_id, dim_supplier.supplier_name, dim_supplier.country) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        _sum(col(\"price\") * col(\"sale_quantity\")).alias(\"weighted_price_sum\"),\n",
    "        _sum(\"sale_quantity\").alias(\"total_quantity\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_price\", col(\"weighted_price_sum\") / col(\"total_quantity\")) \\\n",
    "    .select(\"supplier_id\", \"supplier_name\", \"country\", \"total_revenue\", \"avg_price\")\n",
    "supplier_vitrina.write.jdbc(url=ch_url, table=\"vitrina_supplier_sales\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Топ-5 поставщиков\n",
    "top5_suppliers = supplier_vitrina.orderBy(desc(\"total_revenue\")).limit(5)\n",
    "top5_suppliers.write.jdbc(url=ch_url, table=\"top5_suppliers_by_revenue\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 6. Витрина качества продукции\n",
    "# ===================================================================\n",
    "quality_vitrina = fact.join(dim_product, fact.product_id == dim_product.product_id) \\\n",
    "    .groupBy(dim_product.product_id, dim_product.name) \\\n",
    "    .agg(\n",
    "        first(\"rating\").alias(\"rating\"),\n",
    "        first(\"reviews\").alias(\"review_count\"),\n",
    "        _sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\")\n",
    "    )\n",
    "quality_vitrina.write.jdbc(url=ch_url, table=\"vitrina_product_quality\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "# Корреляция (одна строка — отдельная таблица)\n",
    "correlation = quality_vitrina.agg(\n",
    "    corr(\"rating\", \"total_revenue\").alias(\"corr_rating_revenue\"),\n",
    "    corr(\"rating\", \"total_quantity\").alias(\"corr_rating_quantity\")\n",
    ").withColumn(\"description\", lit(\"Correlation between rating and sales\"))\n",
    "correlation.write.jdbc(url=ch_url, table=\"product_quality_correlation\", mode=\"overwrite\", properties=ch_properties)\n",
    "\n",
    "\n",
    "print(\"Все 6 витрин + топы + корреляция успешно загружены в ClickHouse!\")\n",
    "print(\"Проверить можно в DBeaver или clickhouse-client:\")\n",
    "print(\"SELECT * FROM vitrina_product_sales LIMIT 10;\")\n",
    "print(\"SELECT corr(rating, total_quantity) FROM vitrina_product_quality;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab2290",
   "metadata": {},
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9acc2b76-ce20-4548-80b0-4add4ed00ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e14cf9e-ea44-43ac-ac23-f0103934d8a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:java.lang.Class.forName.\n: java.lang.ClassNotFoundException: clickhouse.DefaultSource\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.Class.forName0(Native Method)\n\tat java.base/java.lang.Class.forName(Unknown Source)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclickhouse.DefaultSource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:java.lang.Class.forName.\n: java.lang.ClassNotFoundException: clickhouse.DefaultSource\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.Class.forName0(Native Method)\n\tat java.base/java.lang.Class.forName(Unknown Source)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "spark._jvm.Class.forName(\"clickhouse.DefaultSource\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d1cf077",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o88.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: clickhouse. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:863)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:257)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.ClassNotFoundException: clickhouse.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 1. Витрина продаж по продуктам\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[1;32m      4\u001b[0m product_vitrina \u001b[38;5;241m=\u001b[39m fact\u001b[38;5;241m.\u001b[39mjoin(dim_product, fact\u001b[38;5;241m.\u001b[39mproduct_id \u001b[38;5;241m==\u001b[39m dim_product\u001b[38;5;241m.\u001b[39mproduct_id) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupBy(dim_product\u001b[38;5;241m.\u001b[39mproduct_id, dim_product\u001b[38;5;241m.\u001b[39mname, dim_product\u001b[38;5;241m.\u001b[39mcategory) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39magg(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         first(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviews\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_count\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     )\n\u001b[0;32m---> 13\u001b[0m \u001b[43mproduct_vitrina\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclickhouse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mch_options\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvitrina_product_sales\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcreateTableOptions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mENGINE = MergeTree() ORDER BY product_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Топ-10 самых продаваемых\u001b[39;00m\n\u001b[1;32m     22\u001b[0m top10_products \u001b[38;5;241m=\u001b[39m product_vitrina\u001b[38;5;241m.\u001b[39morderBy(desc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_quantity\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o88.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: clickhouse. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:863)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:257)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.ClassNotFoundException: clickhouse.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\tat java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 1. Витрина продаж по продуктам\n",
    "# ===============================\n",
    "product_vitrina = fact.join(dim_product, fact.product_id == dim_product.product_id) \\\n",
    "    .groupBy(dim_product.product_id, dim_product.name, dim_product.category) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        first(\"rating\").alias(\"avg_rating\"),\n",
    "        first(\"reviews\").alias(\"review_count\")\n",
    "    )\n",
    "\n",
    "product_vitrina.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"vitrina_product_sales\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY product_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# Топ-10 самых продаваемых\n",
    "top10_products = product_vitrina.orderBy(desc(\"total_quantity\")).limit(10)\n",
    "top10_products.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"top10_sold_products\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY product_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# Выручка по категориям\n",
    "category_revenue = product_vitrina.groupBy(\"category\") \\\n",
    "    .agg(_sum(\"total_revenue\").alias(\"category_revenue\"))\n",
    "category_revenue.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"category_revenue\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY category\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# ===============================\n",
    "# 2. Витрина продаж по клиентам\n",
    "# ===============================\n",
    "customer_vitrina = fact.join(dim_customer, fact.customer_id == dim_customer.customer_id) \\\n",
    "    .groupBy(dim_customer.customer_id, dim_customer.first_name, dim_customer.last_name, dim_customer.country) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_total_price\").alias(\"total_spent\"),\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        avg(\"sale_total_price\").alias(\"avg_check\")\n",
    "    ) \\\n",
    "    .withColumn(\"customer_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\"))) \\\n",
    "    .select(\"customer_id\", \"customer_name\", \"country\", \"total_spent\", \"order_count\", \"avg_check\")\n",
    "\n",
    "customer_vitrina.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"vitrina_customer_sales\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY customer_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "top10_customers = customer_vitrina.orderBy(desc(\"total_spent\")).limit(10)\n",
    "top10_customers.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"top10_customers_by_spent\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY customer_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "customer_country_dist = customer_vitrina.groupBy(\"country\") \\\n",
    "    .agg(\n",
    "        _sum(\"total_spent\").alias(\"total_spent_by_country\"),\n",
    "        count(\"*\").alias(\"customer_count\")\n",
    "    )\n",
    "customer_country_dist.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"customer_country_distribution\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY country\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# ===============================\n",
    "# 3. Витрина продаж по времени\n",
    "# ===============================\n",
    "time_vitrina = fact.join(dim_date, fact.date_id == dim_date.date_id) \\\n",
    "    .groupBy(dim_date.year, dim_date.month) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        _sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        count(\"*\").alias(\"order_count\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_check\", col(\"total_revenue\") / col(\"order_count\")) \\\n",
    "    .withColumn(\"avg_order_size\", col(\"total_quantity\") / col(\"order_count\"))\n",
    "\n",
    "time_vitrina.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"vitrina_time_sales\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY (year, month)\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# ===============================\n",
    "# 4. Витрина продаж по магазинам\n",
    "# ===============================\n",
    "store_vitrina = fact.join(dim_store, fact.store_id == dim_store.store_id) \\\n",
    "    .groupBy(dim_store.store_id, dim_store.store_name, dim_store.city, dim_store.country) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        avg(\"sale_total_price\").alias(\"avg_check\")\n",
    "    )\n",
    "\n",
    "store_vitrina.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"vitrina_store_sales\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY store_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "top5_stores = store_vitrina.orderBy(desc(\"total_revenue\")).limit(5)\n",
    "top5_stores.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"top5_stores_by_revenue\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY store_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# ===============================\n",
    "# 5. Витрина продаж по поставщикам\n",
    "# ===============================\n",
    "supplier_vitrina = fact.join(dim_product[[\"product_id\", \"price\"]], fact.product_id == dim_product.product_id) \\\n",
    "    .join(dim_supplier, fact.supplier_id == dim_supplier.supplier_id) \\\n",
    "    .groupBy(dim_supplier.supplier_id, dim_supplier.supplier_name, dim_supplier.country) \\\n",
    "    .agg(\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        _sum(col(\"price\") * col(\"sale_quantity\")).alias(\"weighted_price_sum\"),\n",
    "        _sum(\"sale_quantity\").alias(\"total_quantity\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_price\", col(\"weighted_price_sum\") / col(\"total_quantity\")) \\\n",
    "    .select(\"supplier_id\", \"supplier_name\", \"country\", \"total_revenue\", \"avg_price\")\n",
    "\n",
    "supplier_vitrina.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"vitrina_supplier_sales\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY supplier_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "top5_suppliers = supplier_vitrina.orderBy(desc(\"total_revenue\")).limit(5)\n",
    "top5_suppliers.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"top5_suppliers_by_revenue\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY supplier_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# ===============================\n",
    "# 6. Витрина качества продукции\n",
    "# ===============================\n",
    "quality_vitrina = fact.join(dim_product, fact.product_id == dim_product.product_id) \\\n",
    "    .groupBy(dim_product.product_id, dim_product.name) \\\n",
    "    .agg(\n",
    "        first(\"rating\").alias(\"rating\"),\n",
    "        first(\"reviews\").alias(\"review_count\"),\n",
    "        _sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        _sum(\"sale_total_price\").alias(\"total_revenue\")\n",
    "    )\n",
    "\n",
    "quality_vitrina.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"vitrina_product_quality\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY product_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# Корреляция\n",
    "correlation = quality_vitrina.agg(\n",
    "    corr(\"rating\", \"total_revenue\").alias(\"corr_rating_revenue\"),\n",
    "    corr(\"rating\", \"total_quantity\").alias(\"corr_rating_quantity\")\n",
    ").withColumn(\"description\", lit(\"Correlation between rating and sales\"))\n",
    "\n",
    "correlation.write \\\n",
    "    .format(\"clickhouse\") \\\n",
    "    .options(**ch_options) \\\n",
    "    .option(\"table\", \"product_quality_correlation\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE = MergeTree() ORDER BY description\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Все 6 витрин + топы + корреляция успешно загружены в ClickHouse!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d53b82cc-1050-47e2-90b7-abb408d78ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Завершаем сессию Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d78a0f-d9d9-40b4-bc09-14a5023c5b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создание витрины продаж по продуктам...\n",
      "✗ Ошибка при создании таблицы vitrina_product_sales: An error occurred while calling o546.save.\n",
      ": java.sql.SQLException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      ", server ClickHouseNode [uri=http://clickhouse:8123/default]@-7368705\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:122)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeLargeUpdate(ClickHouseStatementImpl.java:489)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeUpdate(ClickHouseStatementImpl.java:498)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.createTable(JdbcDialects.scala:192)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:923)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      "\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)\n",
      "\tat com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)\n",
      "\tat com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)\n",
      "\tat com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)\n",
      "\tat com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:120)\n",
      "\t... 46 more\n",
      "\n",
      "✗ Ошибка при создании таблицы top10_sold_products: An error occurred while calling o563.save.\n",
      ": java.sql.SQLException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      ", server ClickHouseNode [uri=http://clickhouse:8123/default]@-7368705\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:122)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeLargeUpdate(ClickHouseStatementImpl.java:489)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeUpdate(ClickHouseStatementImpl.java:498)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.createTable(JdbcDialects.scala:192)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:923)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      "\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)\n",
      "\tat com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)\n",
      "\tat com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)\n",
      "\tat com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)\n",
      "\tat com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:120)\n",
      "\t... 46 more\n",
      "\n",
      "✗ Ошибка при создании таблицы category_revenue: An error occurred while calling o585.save.\n",
      ": java.sql.SQLException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      ", server ClickHouseNode [uri=http://clickhouse:8123/default]@-7368705\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:122)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeLargeUpdate(ClickHouseStatementImpl.java:489)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeUpdate(ClickHouseStatementImpl.java:498)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.createTable(JdbcDialects.scala:192)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:923)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      "\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)\n",
      "\tat com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)\n",
      "\tat com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)\n",
      "\tat com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)\n",
      "\tat com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:120)\n",
      "\t... 45 more\n",
      "\n",
      "Создание витрины продаж по клиентам...\n",
      "✗ Ошибка при создании таблицы vitrina_customer_sales: An error occurred while calling o635.save.\n",
      ": java.sql.SQLException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      ", server ClickHouseNode [uri=http://clickhouse:8123/default]@-7368705\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:122)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeLargeUpdate(ClickHouseStatementImpl.java:489)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeUpdate(ClickHouseStatementImpl.java:498)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.createTable(JdbcDialects.scala:192)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:923)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      "\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)\n",
      "\tat com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)\n",
      "\tat com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)\n",
      "\tat com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)\n",
      "\tat com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:120)\n",
      "\t... 45 more\n",
      "\n",
      "✗ Ошибка при создании таблицы top10_customers_by_spent: An error occurred while calling o652.save.\n",
      ": java.sql.SQLException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      ", server ClickHouseNode [uri=http://clickhouse:8123/default]@-7368705\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:122)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeLargeUpdate(ClickHouseStatementImpl.java:489)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeUpdate(ClickHouseStatementImpl.java:498)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.createTable(JdbcDialects.scala:192)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:923)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      "\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)\n",
      "\tat com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)\n",
      "\tat com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)\n",
      "\tat com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)\n",
      "\tat com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:120)\n",
      "\t... 45 more\n",
      "\n",
      "✗ Ошибка при создании таблицы customer_country_distribution: An error occurred while calling o677.save.\n",
      ": java.sql.SQLException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      ", server ClickHouseNode [uri=http://clickhouse:8123/default]@-7368705\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:122)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeLargeUpdate(ClickHouseStatementImpl.java:489)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeUpdate(ClickHouseStatementImpl.java:498)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.createTable(JdbcDialects.scala:192)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:923)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      "\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)\n",
      "\tat com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)\n",
      "\tat com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)\n",
      "\tat com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)\n",
      "\tat com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:120)\n",
      "\t... 45 more\n",
      "\n",
      "Создание витрины продаж по времени...\n",
      "✗ Ошибка при создании таблицы vitrina_time_sales: An error occurred while calling o718.save.\n",
      ": java.sql.SQLException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      ", server ClickHouseNode [uri=http://clickhouse:8123/default]@-7368705\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:122)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeLargeUpdate(ClickHouseStatementImpl.java:489)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeUpdate(ClickHouseStatementImpl.java:498)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.createTable(JdbcDialects.scala:192)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:923)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      "\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)\n",
      "\tat com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)\n",
      "\tat com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)\n",
      "\tat com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)\n",
      "\tat com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:120)\n",
      "\t... 45 more\n",
      "\n",
      "Создание витрины продаж по магазинам...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'city'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 118\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# 4. Витрина продаж по магазинам\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mСоздание витрины продаж по магазинам...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    117\u001b[0m store_vitrina \u001b[38;5;241m=\u001b[39m fact\u001b[38;5;241m.\u001b[39mjoin(dim_store, fact\u001b[38;5;241m.\u001b[39mstore_id \u001b[38;5;241m==\u001b[39m dim_store\u001b[38;5;241m.\u001b[39mstore_id) \\\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupBy(dim_store\u001b[38;5;241m.\u001b[39mstore_id, dim_store\u001b[38;5;241m.\u001b[39mstore_name, \u001b[43mdim_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcity\u001b[49m, dim_store\u001b[38;5;241m.\u001b[39mcountry) \\\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;241m.\u001b[39magg(\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28msum\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msale_total_price\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_revenue\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    121\u001b[0m         count(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder_count\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    122\u001b[0m         avg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msale_total_price\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_check\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    123\u001b[0m     )\n\u001b[1;32m    125\u001b[0m write_to_clickhouse(store_vitrina, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvitrina_store_sales\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    127\u001b[0m top5_stores \u001b[38;5;241m=\u001b[39m store_vitrina\u001b[38;5;241m.\u001b[39morderBy(desc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_revenue\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:3129\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3096\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   3097\u001b[0m \n\u001b[1;32m   3098\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3126\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[1;32m   3127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 3129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   3130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   3131\u001b[0m     )\n\u001b[1;32m   3132\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   3133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'city'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import math\n",
    "\n",
    "# ===============================\n",
    "# Настройки подключения к ClickHouse\n",
    "# ===============================\n",
    "ch_jdbc_url = \"jdbc:clickhouse://clickhouse:8123/default\"\n",
    "ch_properties = {\n",
    "    \"user\": \"default\",\n",
    "    \"password\": \"\",\n",
    "    \"driver\": \"com.clickhouse.jdbc.ClickHouseDriver\"\n",
    "}\n",
    "\n",
    "# Батчинг для больших данных\n",
    "batch_size = 100000\n",
    "\n",
    "# ===============================\n",
    "# Вспомогательная функция для записи через JDBC\n",
    "# ===============================\n",
    "def write_to_clickhouse(df, table_name, order_by_columns=None):\n",
    "    \"\"\"Запись DataFrame в ClickHouse через JDBC с батчингом\"\"\"\n",
    "    \n",
    "    # Создаем опции для создания таблицы\n",
    "    create_table_options = \"\"\n",
    "    if order_by_columns:\n",
    "        create_table_options = f\"ORDER BY ({', '.join(order_by_columns)})\"\n",
    "    \n",
    "    try:\n",
    "        # Записываем данные\n",
    "        df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", ch_jdbc_url) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"user\", ch_properties[\"user\"]) \\\n",
    "            .option(\"password\", ch_properties[\"password\"]) \\\n",
    "            .option(\"driver\", ch_properties[\"driver\"]) \\\n",
    "            .option(\"batchsize\", batch_size) \\\n",
    "            .option(\"createTableOptions\", create_table_options) \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "        \n",
    "        print(f\"✓ Таблица {table_name} успешно создана\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ошибка при создании таблицы {table_name}: {e}\")\n",
    "\n",
    "# ===============================\n",
    "# 1. Витрина продаж по продуктам\n",
    "# ===============================\n",
    "print(\"Создание витрины продаж по продуктам...\")\n",
    "product_vitrina = fact.join(dim_product, fact.product_id == dim_product.product_id) \\\n",
    "    .groupBy(dim_product.product_id, dim_product.name, dim_product.category) \\\n",
    "    .agg(\n",
    "        sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        first(\"rating\").alias(\"avg_rating\"),\n",
    "        first(\"reviews\").alias(\"review_count\")\n",
    "    )\n",
    "\n",
    "write_to_clickhouse(product_vitrina, \"vitrina_product_sales\", [\"product_id\"])\n",
    "\n",
    "# Топ-10 самых продаваемых\n",
    "top10_products = product_vitrina.orderBy(desc(\"total_quantity\")).limit(10)\n",
    "write_to_clickhouse(top10_products, \"top10_sold_products\", [\"product_id\"])\n",
    "\n",
    "# Выручка по категориям\n",
    "category_revenue = product_vitrina.groupBy(\"category\") \\\n",
    "    .agg(sum(\"total_revenue\").alias(\"category_revenue\"))\n",
    "write_to_clickhouse(category_revenue, \"category_revenue\", [\"category\"])\n",
    "\n",
    "# ===============================\n",
    "# 2. Витрина продаж по клиентам\n",
    "# ===============================\n",
    "print(\"Создание витрины продаж по клиентам...\")\n",
    "customer_vitrina = fact.join(dim_customer, fact.customer_id == dim_customer.customer_id) \\\n",
    "    .groupBy(dim_customer.customer_id, dim_customer.first_name, dim_customer.last_name, dim_customer.country) \\\n",
    "    .agg(\n",
    "        sum(\"sale_total_price\").alias(\"total_spent\"),\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        avg(\"sale_total_price\").alias(\"avg_check\")\n",
    "    ) \\\n",
    "    .withColumn(\"customer_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\"))) \\\n",
    "    .select(\"customer_id\", \"customer_name\", \"country\", \"total_spent\", \"order_count\", \"avg_check\")\n",
    "\n",
    "write_to_clickhouse(customer_vitrina, \"vitrina_customer_sales\", [\"customer_id\"])\n",
    "\n",
    "top10_customers = customer_vitrina.orderBy(desc(\"total_spent\")).limit(10)\n",
    "write_to_clickhouse(top10_customers, \"top10_customers_by_spent\", [\"customer_id\"])\n",
    "\n",
    "customer_country_dist = customer_vitrina.groupBy(\"country\") \\\n",
    "    .agg(\n",
    "        sum(\"total_spent\").alias(\"total_spent_by_country\"),\n",
    "        count(\"*\").alias(\"customer_count\")\n",
    "    )\n",
    "write_to_clickhouse(customer_country_dist, \"customer_country_distribution\", [\"country\"])\n",
    "\n",
    "# ===============================\n",
    "# 3. Витрина продаж по времени\n",
    "# ===============================\n",
    "print(\"Создание витрины продаж по времени...\")\n",
    "time_vitrina = fact.join(dim_date, fact.date_id == dim_date.date_id) \\\n",
    "    .groupBy(dim_date.year, dim_date.month) \\\n",
    "    .agg(\n",
    "        sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        count(\"*\").alias(\"order_count\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_check\", col(\"total_revenue\") / col(\"order_count\")) \\\n",
    "    .withColumn(\"avg_order_size\", col(\"total_quantity\") / col(\"order_count\"))\n",
    "\n",
    "write_to_clickhouse(time_vitrina, \"vitrina_time_sales\", [\"year\", \"month\"])\n",
    "\n",
    "# ===============================\n",
    "# 4. Витрина продаж по магазинам\n",
    "# ===============================\n",
    "print(\"Создание витрины продаж по магазинам...\")\n",
    "store_vitrina = fact.join(dim_store, fact.store_id == dim_store.store_id) \\\n",
    "    .groupBy(dim_store.store_id, dim_store.store_name, dim_store.city, dim_store.country) \\\n",
    "    .agg(\n",
    "        sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        avg(\"sale_total_price\").alias(\"avg_check\")\n",
    "    )\n",
    "\n",
    "write_to_clickhouse(store_vitrina, \"vitrina_store_sales\", [\"store_id\"])\n",
    "\n",
    "top5_stores = store_vitrina.orderBy(desc(\"total_revenue\")).limit(5)\n",
    "write_to_clickhouse(top5_stores, \"top5_stores_by_revenue\", [\"store_id\"])\n",
    "\n",
    "# ===============================\n",
    "# 5. Витрина продаж по поставщикам\n",
    "# ===============================\n",
    "print(\"Создание витрины продаж по поставщикам...\")\n",
    "supplier_vitrina = fact.join(dim_product, fact.product_id == dim_product.product_id) \\\n",
    "    .join(dim_supplier, fact.supplier_id == dim_supplier.supplier_id) \\\n",
    "    .groupBy(dim_supplier.supplier_id, dim_supplier.supplier_name, dim_supplier.country) \\\n",
    "    .agg(\n",
    "        sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        sum(col(\"price\") * col(\"sale_quantity\")).alias(\"weighted_price_sum\"),\n",
    "        sum(\"sale_quantity\").alias(\"total_quantity\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_price\", col(\"weighted_price_sum\") / col(\"total_quantity\")) \\\n",
    "    .select(\"supplier_id\", \"supplier_name\", \"country\", \"total_revenue\", \"avg_price\")\n",
    "\n",
    "write_to_clickhouse(supplier_vitrina, \"vitrina_supplier_sales\", [\"supplier_id\"])\n",
    "\n",
    "top5_suppliers = supplier_vitrina.orderBy(desc(\"total_revenue\")).limit(5)\n",
    "write_to_clickhouse(top5_suppliers, \"top5_suppliers_by_revenue\", [\"supplier_id\"])\n",
    "\n",
    "# ===============================\n",
    "# 6. Витрина качества продукции\n",
    "# ===============================\n",
    "print(\"Создание витрины качества продукции...\")\n",
    "quality_vitrina = fact.join(dim_product, fact.product_id == dim_product.product_id) \\\n",
    "    .groupBy(dim_product.product_id, dim_product.name) \\\n",
    "    .agg(\n",
    "        first(\"rating\").alias(\"rating\"),\n",
    "        first(\"reviews\").alias(\"review_count\"),\n",
    "        sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        sum(\"sale_total_price\").alias(\"total_revenue\")\n",
    "    )\n",
    "\n",
    "write_to_clickhouse(quality_vitrina, \"vitrina_product_quality\", [\"product_id\"])\n",
    "\n",
    "# Корреляция\n",
    "correlation = quality_vitrina.agg(\n",
    "    corr(\"rating\", \"total_revenue\").alias(\"corr_rating_revenue\"),\n",
    "    corr(\"rating\", \"total_quantity\").alias(\"corr_rating_quantity\")\n",
    ").withColumn(\"description\", lit(\"Correlation between rating and sales\"))\n",
    "\n",
    "write_to_clickhouse(correlation, \"product_quality_correlation\", [\"description\"])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ВСЕ 6 ВИТРИН + ТОПЫ + КОРРЕЛЯЦИЯ УСПЕШНО ЗАГРУЖЕНЫ В CLICKHOUSE!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ===============================\n",
    "# Дополнительно: проверка данных\n",
    "# ===============================\n",
    "def check_table_count(table_name):\n",
    "    \"\"\"Проверка количества записей в таблице\"\"\"\n",
    "    try:\n",
    "        count_df = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", ch_jdbc_url) \\\n",
    "            .option(\"dbtable\", f\"(SELECT count(*) as cnt FROM {table_name}) as t\") \\\n",
    "            .option(\"user\", ch_properties[\"user\"]) \\\n",
    "            .option(\"password\", ch_properties[\"password\"]) \\\n",
    "            .option(\"driver\", ch_properties[\"driver\"]) \\\n",
    "            .load()\n",
    "        count = count_df.first()[\"cnt\"]\n",
    "        print(f\"✓ Таблица {table_name}: {count} записей\")\n",
    "        return count\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ошибка при проверке таблицы {table_name}: {e}\")\n",
    "        return 0\n",
    "\n",
    "print(\"\\nПроверка загруженных данных:\")\n",
    "tables_to_check = [\n",
    "    \"vitrina_product_sales\", \"vitrina_customer_sales\", \"vitrina_time_sales\",\n",
    "    \"vitrina_store_sales\", \"vitrina_supplier_sales\", \"vitrina_product_quality\"\n",
    "]\n",
    "\n",
    "for table in tables_to_check:\n",
    "    check_table_count(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc01053e-e11f-490d-8473-1f8f3d94b6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создание витрины продаж по продуктам...\n",
      "✗ Ошибка очистки vitrina_product_sales: Code: 60. DB::Exception: Table default.vitrina_product_sales doesn't exist. (UNKNOWN_TABLE) (version 23.7.6.111 (official build))\n",
      "\n",
      "✗ Ошибка при записи в таблицу vitrina_product_sales: An error occurred while calling o762.save.\n",
      ": java.sql.SQLException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      ", server ClickHouseNode [uri=http://clickhouse:8123/default]@-7368705\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:122)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeLargeUpdate(ClickHouseStatementImpl.java:489)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeUpdate(ClickHouseStatementImpl.java:498)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.createTable(JdbcDialects.scala:192)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:923)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      "\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)\n",
      "\tat com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)\n",
      "\tat com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)\n",
      "\tat com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)\n",
      "\tat com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:120)\n",
      "\t... 45 more\n",
      "\n",
      "✗ Ошибка очистки top10_sold_products: Code: 60. DB::Exception: Table default.top10_sold_products doesn't exist. (UNKNOWN_TABLE) (version 23.7.6.111 (official build))\n",
      "\n",
      "✗ Ошибка при записи в таблицу top10_sold_products: An error occurred while calling o778.save.\n",
      ": java.sql.SQLException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      ", server ClickHouseNode [uri=http://clickhouse:8123/default]@-7368705\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:122)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeLargeUpdate(ClickHouseStatementImpl.java:489)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeUpdate(ClickHouseStatementImpl.java:498)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.createTable(JdbcDialects.scala:192)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:923)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      "\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)\n",
      "\tat com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)\n",
      "\tat com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)\n",
      "\tat com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)\n",
      "\tat com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:120)\n",
      "\t... 45 more\n",
      "\n",
      "✗ Ошибка очистки category_revenue: Code: 60. DB::Exception: Table default.category_revenue doesn't exist. (UNKNOWN_TABLE) (version 23.7.6.111 (official build))\n",
      "\n",
      "✗ Ошибка при записи в таблицу category_revenue: An error occurred while calling o799.save.\n",
      ": java.sql.SQLException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      ", server ClickHouseNode [uri=http://clickhouse:8123/default]@-7368705\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:122)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeLargeUpdate(ClickHouseStatementImpl.java:489)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeUpdate(ClickHouseStatementImpl.java:498)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.createTable(JdbcDialects.scala:192)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:923)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      "\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)\n",
      "\tat com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)\n",
      "\tat com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)\n",
      "\tat com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)\n",
      "\tat com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:120)\n",
      "\t... 45 more\n",
      "\n",
      "Создание витрины продаж по клиентам...\n",
      "✗ Ошибка очистки vitrina_customer_sales: Code: 60. DB::Exception: Table default.vitrina_customer_sales doesn't exist. (UNKNOWN_TABLE) (version 23.7.6.111 (official build))\n",
      "\n",
      "✗ Ошибка при записи в таблицу vitrina_customer_sales: An error occurred while calling o848.save.\n",
      ": java.sql.SQLException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      ", server ClickHouseNode [uri=http://clickhouse:8123/default]@-7368705\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:122)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeLargeUpdate(ClickHouseStatementImpl.java:489)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeUpdate(ClickHouseStatementImpl.java:498)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.createTable(JdbcDialects.scala:192)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:923)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      "\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)\n",
      "\tat com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)\n",
      "\tat com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)\n",
      "\tat com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)\n",
      "\tat com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:120)\n",
      "\t... 45 more\n",
      "\n",
      "✗ Ошибка очистки top10_customers_by_spent: Code: 60. DB::Exception: Table default.top10_customers_by_spent doesn't exist. (UNKNOWN_TABLE) (version 23.7.6.111 (official build))\n",
      "\n",
      "✗ Ошибка при записи в таблицу top10_customers_by_spent: An error occurred while calling o864.save.\n",
      ": java.sql.SQLException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      ", server ClickHouseNode [uri=http://clickhouse:8123/default]@-7368705\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:122)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeLargeUpdate(ClickHouseStatementImpl.java:489)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeUpdate(ClickHouseStatementImpl.java:498)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.createTable(JdbcDialects.scala:192)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:923)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      "\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)\n",
      "\tat com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)\n",
      "\tat com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)\n",
      "\tat com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)\n",
      "\tat com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:120)\n",
      "\t... 45 more\n",
      "\n",
      "✗ Ошибка очистки customer_country_distribution: Code: 60. DB::Exception: Table default.customer_country_distribution doesn't exist. (UNKNOWN_TABLE) (version 23.7.6.111 (official build))\n",
      "\n",
      "✗ Ошибка при записи в таблицу customer_country_distribution: An error occurred while calling o888.save.\n",
      ": java.sql.SQLException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      ", server ClickHouseNode [uri=http://clickhouse:8123/default]@-7368705\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:122)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeLargeUpdate(ClickHouseStatementImpl.java:489)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeUpdate(ClickHouseStatementImpl.java:498)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.createTable(JdbcDialects.scala:192)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:923)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      "\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)\n",
      "\tat com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)\n",
      "\tat com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)\n",
      "\tat com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)\n",
      "\tat com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:120)\n",
      "\t... 45 more\n",
      "\n",
      "Создание витрины продаж по времени...\n",
      "✗ Ошибка очистки vitrina_time_sales: Code: 60. DB::Exception: Table default.vitrina_time_sales doesn't exist. (UNKNOWN_TABLE) (version 23.7.6.111 (official build))\n",
      "\n",
      "✗ Ошибка при записи в таблицу vitrina_time_sales: An error occurred while calling o928.save.\n",
      ": java.sql.SQLException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      ", server ClickHouseNode [uri=http://clickhouse:8123/default]@-7368705\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:85)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.create(SqlExceptionUtils.java:31)\n",
      "\tat com.clickhouse.jdbc.SqlExceptionUtils.handle(SqlExceptionUtils.java:90)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:122)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeLargeUpdate(ClickHouseStatementImpl.java:489)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeUpdate(ClickHouseStatementImpl.java:498)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.createTable(JdbcDialects.scala:192)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:923)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Code: 119. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED) (version 23.7.6.111 (official build))\n",
      "\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.checkResponse(HttpUrlConnectionImpl.java:184)\n",
      "\tat com.clickhouse.client.http.HttpUrlConnectionImpl.post(HttpUrlConnectionImpl.java:227)\n",
      "\tat com.clickhouse.client.http.ClickHouseHttpClient.send(ClickHouseHttpClient.java:124)\n",
      "\tat com.clickhouse.client.AbstractClient.execute(AbstractClient.java:280)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.sendOnce(ClickHouseClientBuilder.java:282)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.send(ClickHouseClientBuilder.java:294)\n",
      "\tat com.clickhouse.client.ClickHouseClientBuilder$Agent.execute(ClickHouseClientBuilder.java:349)\n",
      "\tat com.clickhouse.client.ClickHouseClient.executeAndWait(ClickHouseClient.java:1056)\n",
      "\tat com.clickhouse.client.ClickHouseRequest.executeAndWait(ClickHouseRequest.java:2154)\n",
      "\tat com.clickhouse.jdbc.internal.ClickHouseStatementImpl.getLastResponse(ClickHouseStatementImpl.java:120)\n",
      "\t... 45 more\n",
      "\n",
      "Создание витрины продаж по магазинам...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'city'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 138\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# 4. Витрина продаж по магазинам\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mСоздание витрины продаж по магазинам...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    137\u001b[0m store_vitrina \u001b[38;5;241m=\u001b[39m fact\u001b[38;5;241m.\u001b[39mjoin(dim_store, fact\u001b[38;5;241m.\u001b[39mstore_id \u001b[38;5;241m==\u001b[39m dim_store\u001b[38;5;241m.\u001b[39mstore_id) \\\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupBy(dim_store\u001b[38;5;241m.\u001b[39mstore_id, dim_store\u001b[38;5;241m.\u001b[39mstore_name, \u001b[43mdim_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcity\u001b[49m, dim_store\u001b[38;5;241m.\u001b[39mcountry) \\\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;241m.\u001b[39magg(\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28msum\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msale_total_price\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_revenue\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    141\u001b[0m         count(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder_count\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    142\u001b[0m         avg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msale_total_price\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_check\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    145\u001b[0m clear_clickhouse_table(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvitrina_store_sales\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    146\u001b[0m write_to_clickhouse_existing(store_vitrina, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvitrina_store_sales\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:3129\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3096\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   3097\u001b[0m \n\u001b[1;32m   3098\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3126\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[1;32m   3127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 3129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   3130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   3131\u001b[0m     )\n\u001b[1;32m   3132\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   3133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'city'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# ===============================\n",
    "# Настройки подключения к ClickHouse\n",
    "# ===============================\n",
    "ch_jdbc_url = \"jdbc:clickhouse://clickhouse:8123/default\"\n",
    "ch_properties = {\n",
    "    \"user\": \"default\",\n",
    "    \"password\": \"\",\n",
    "    \"driver\": \"com.clickhouse.jdbc.ClickHouseDriver\"\n",
    "}\n",
    "\n",
    "# Батчинг для больших данных\n",
    "batch_size = 100000\n",
    "\n",
    "# ===============================\n",
    "# Вспомогательная функция для записи в существующие таблицы\n",
    "# ===============================\n",
    "def write_to_clickhouse_existing(df, table_name):\n",
    "    \"\"\"Запись DataFrame в существующую таблицу ClickHouse\"\"\"\n",
    "    try:\n",
    "        # Очищаем таблицу перед записью новых данных\n",
    "        clear_table_query = f\"TRUNCATE TABLE {table_name}\"\n",
    "        # Выполняем через Spark SQL или другим способом\n",
    "        \n",
    "        # Записываем данные в существующую таблицу\n",
    "        df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", ch_jdbc_url) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"user\", ch_properties[\"user\"]) \\\n",
    "            .option(\"password\", ch_properties[\"password\"]) \\\n",
    "            .option(\"driver\", ch_properties[\"driver\"]) \\\n",
    "            .option(\"batchsize\", batch_size) \\\n",
    "            .mode(\"append\").save()\n",
    "        \n",
    "        print(f\"✓ Данные записаны в таблицу {table_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ошибка при записи в таблицу {table_name}: {e}\")\n",
    "\n",
    "# ===============================\n",
    "# Функция для очистки таблиц через ClickHouse HTTP API\n",
    "# ===============================\n",
    "def clear_clickhouse_table(table_name):\n",
    "    \"\"\"Очистка таблицы через HTTP запрос\"\"\"\n",
    "    try:\n",
    "        import requests\n",
    "        query = f\"TRUNCATE TABLE {table_name}\"\n",
    "        url = \"http://clickhouse:8123/\"\n",
    "        response = requests.post(url, data=query)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"✓ Таблица {table_name} очищена\")\n",
    "        else:\n",
    "            print(f\"✗ Ошибка очистки {table_name}: {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ошибка при очистке таблицы {table_name}: {e}\")\n",
    "\n",
    "# ===============================\n",
    "# 1. Витрина продаж по продуктам\n",
    "# ===============================\n",
    "print(\"Создание витрины продаж по продуктам...\")\n",
    "product_vitrina = fact.join(dim_product, fact.product_id == dim_product.product_id) \\\n",
    "    .groupBy(dim_product.product_id, dim_product.name, dim_product.category) \\\n",
    "    .agg(\n",
    "        sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        first(\"rating\").alias(\"avg_rating\"),\n",
    "        first(\"reviews\").alias(\"review_count\")\n",
    "    )\n",
    "\n",
    "# Очищаем и записываем\n",
    "clear_clickhouse_table(\"vitrina_product_sales\")\n",
    "write_to_clickhouse_existing(product_vitrina, \"vitrina_product_sales\")\n",
    "\n",
    "# Топ-10 самых продаваемых\n",
    "top10_products = product_vitrina.orderBy(desc(\"total_quantity\")).limit(10)\n",
    "clear_clickhouse_table(\"top10_sold_products\")\n",
    "write_to_clickhouse_existing(top10_products, \"top10_sold_products\")\n",
    "\n",
    "# Выручка по категориям\n",
    "category_revenue = product_vitrina.groupBy(\"category\") \\\n",
    "    .agg(sum(\"total_revenue\").alias(\"category_revenue\"))\n",
    "clear_clickhouse_table(\"category_revenue\")\n",
    "write_to_clickhouse_existing(category_revenue, \"category_revenue\")\n",
    "\n",
    "# ===============================\n",
    "# 2. Витрина продаж по клиентам\n",
    "# ===============================\n",
    "print(\"Создание витрины продаж по клиентам...\")\n",
    "customer_vitrina = fact.join(dim_customer, fact.customer_id == dim_customer.customer_id) \\\n",
    "    .groupBy(dim_customer.customer_id, dim_customer.first_name, dim_customer.last_name, dim_customer.country) \\\n",
    "    .agg(\n",
    "        sum(\"sale_total_price\").alias(\"total_spent\"),\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        avg(\"sale_total_price\").alias(\"avg_check\")\n",
    "    ) \\\n",
    "    .withColumn(\"customer_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\"))) \\\n",
    "    .select(\"customer_id\", \"customer_name\", \"country\", \"total_spent\", \"order_count\", \"avg_check\")\n",
    "\n",
    "clear_clickhouse_table(\"vitrina_customer_sales\")\n",
    "write_to_clickhouse_existing(customer_vitrina, \"vitrina_customer_sales\")\n",
    "\n",
    "top10_customers = customer_vitrina.orderBy(desc(\"total_spent\")).limit(10)\n",
    "clear_clickhouse_table(\"top10_customers_by_spent\")\n",
    "write_to_clickhouse_existing(top10_customers, \"top10_customers_by_spent\")\n",
    "\n",
    "customer_country_dist = customer_vitrina.groupBy(\"country\") \\\n",
    "    .agg(\n",
    "        sum(\"total_spent\").alias(\"total_spent_by_country\"),\n",
    "        count(\"*\").alias(\"customer_count\")\n",
    "    )\n",
    "clear_clickhouse_table(\"customer_country_distribution\")\n",
    "write_to_clickhouse_existing(customer_country_dist, \"customer_country_distribution\")\n",
    "\n",
    "# ===============================\n",
    "# 3. Витрина продаж по времени\n",
    "# ===============================\n",
    "print(\"Создание витрины продаж по времени...\")\n",
    "time_vitrina = fact.join(dim_date, fact.date_id == dim_date.date_id) \\\n",
    "    .groupBy(dim_date.year, dim_date.month) \\\n",
    "    .agg(\n",
    "        sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        count(\"*\").alias(\"order_count\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_check\", col(\"total_revenue\") / col(\"order_count\")) \\\n",
    "    .withColumn(\"avg_order_size\", col(\"total_quantity\") / col(\"order_count\"))\n",
    "\n",
    "clear_clickhouse_table(\"vitrina_time_sales\")\n",
    "write_to_clickhouse_existing(time_vitrina, \"vitrina_time_sales\")\n",
    "\n",
    "# ===============================\n",
    "# 4. Витрина продаж по магазинам\n",
    "# ===============================\n",
    "print(\"Создание витрины продаж по магазинам...\")\n",
    "store_vitrina = fact.join(dim_store, fact.store_id == dim_store.store_id) \\\n",
    "    .groupBy(dim_store.store_id, dim_store.store_name, dim_store.city, dim_store.country) \\\n",
    "    .agg(\n",
    "        sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        avg(\"sale_total_price\").alias(\"avg_check\")\n",
    "    )\n",
    "\n",
    "clear_clickhouse_table(\"vitrina_store_sales\")\n",
    "write_to_clickhouse_existing(store_vitrina, \"vitrina_store_sales\")\n",
    "\n",
    "top5_stores = store_vitrina.orderBy(desc(\"total_revenue\")).limit(5)\n",
    "clear_clickhouse_table(\"top5_stores_by_revenue\")\n",
    "write_to_clickhouse_existing(top5_stores, \"top5_stores_by_revenue\")\n",
    "\n",
    "# ===============================\n",
    "# 5. Витрина продаж по поставщикам\n",
    "# ===============================\n",
    "print(\"Создание витрины продаж по поставщикам...\")\n",
    "supplier_vitrina = fact.join(dim_product, fact.product_id == dim_product.product_id) \\\n",
    "    .join(dim_supplier, fact.supplier_id == dim_supplier.supplier_id) \\\n",
    "    .groupBy(dim_supplier.supplier_id, dim_supplier.supplier_name, dim_supplier.country) \\\n",
    "    .agg(\n",
    "        sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        sum(col(\"price\") * col(\"sale_quantity\")).alias(\"weighted_price_sum\"),\n",
    "        sum(\"sale_quantity\").alias(\"total_quantity\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_price\", col(\"weighted_price_sum\") / col(\"total_quantity\")) \\\n",
    "    .select(\"supplier_id\", \"supplier_name\", \"country\", \"total_revenue\", \"avg_price\")\n",
    "\n",
    "clear_clickhouse_table(\"vitrina_supplier_sales\")\n",
    "write_to_clickhouse_existing(supplier_vitrina, \"vitrina_supplier_sales\")\n",
    "\n",
    "top5_suppliers = supplier_vitrina.orderBy(desc(\"total_revenue\")).limit(5)\n",
    "clear_clickhouse_table(\"top5_suppliers_by_revenue\")\n",
    "write_to_clickhouse_existing(top5_suppliers, \"top5_suppliers_by_revenue\")\n",
    "\n",
    "# ===============================\n",
    "# 6. Витрина качества продукции\n",
    "# ===============================\n",
    "print(\"Создание витрины качества продукции...\")\n",
    "quality_vitrina = fact.join(dim_product, fact.product_id == dim_product.product_id) \\\n",
    "    .groupBy(dim_product.product_id, dim_product.name) \\\n",
    "    .agg(\n",
    "        first(\"rating\").alias(\"rating\"),\n",
    "        first(\"reviews\").alias(\"review_count\"),\n",
    "        sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        sum(\"sale_total_price\").alias(\"total_revenue\")\n",
    "    )\n",
    "\n",
    "clear_clickhouse_table(\"vitrina_product_quality\")\n",
    "write_to_clickhouse_existing(quality_vitrina, \"vitrina_product_quality\")\n",
    "\n",
    "# Корреляция\n",
    "correlation = quality_vitrina.agg(\n",
    "    corr(\"rating\", \"total_revenue\").alias(\"corr_rating_revenue\"),\n",
    "    corr(\"rating\", \"total_quantity\").alias(\"corr_rating_quantity\")\n",
    ").withColumn(\"description\", lit(\"Correlation between rating and sales\"))\n",
    "\n",
    "clear_clickhouse_table(\"product_quality_correlation\")\n",
    "write_to_clickhouse_existing(correlation, \"product_quality_correlation\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ВСЕ ДАННЫЕ УСПЕШНО ЗАГРУЖЕНЫ В СУЩЕСТВУЮЩИЕ ТАБЛИЦЫ CLICKHOUSE!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ===============================\n",
    "# Проверка данных\n",
    "# ===============================\n",
    "def check_table_count(table_name):\n",
    "    \"\"\"Проверка количества записей в таблице\"\"\"\n",
    "    try:\n",
    "        count_df = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", ch_jdbc_url) \\\n",
    "            .option(\"dbtable\", f\"(SELECT count(*) as cnt FROM {table_name}) as t\") \\\n",
    "            .option(\"user\", ch_properties[\"user\"]) \\\n",
    "            .option(\"password\", ch_properties[\"password\"]) \\\n",
    "            .option(\"driver\", ch_properties[\"driver\"]) \\\n",
    "            .load()\n",
    "        count = count_df.first()[\"cnt\"]\n",
    "        print(f\"✓ Таблица {table_name}: {count} записей\")\n",
    "        return count\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ошибка при проверке таблицы {table_name}: {e}\")\n",
    "        return 0\n",
    "\n",
    "print(\"\\nПроверка загруженных данных:\")\n",
    "tables_to_check = [\n",
    "    \"vitrina_product_sales\", \"vitrina_customer_sales\", \"vitrina_time_sales\",\n",
    "    \"vitrina_store_sales\", \"vitrina_supplier_sales\", \"vitrina_product_quality\",\n",
    "    \"top10_sold_products\", \"top10_customers_by_spent\", \"top5_stores_by_revenue\",\n",
    "    \"top5_suppliers_by_revenue\", \"product_quality_correlation\"\n",
    "]\n",
    "\n",
    "for table in tables_to_check:\n",
    "    check_table_count(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "974ea5e1-1b92-4e4a-9c79-b8da6e73f282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создание таблиц в ClickHouse...\n",
      "✓ Таблица vitrina_product_sales создана\n",
      "✓ Таблица vitrina_customer_sales создана\n",
      "✓ Таблица vitrina_time_sales создана\n",
      "✓ Таблица vitrina_store_sales создана\n",
      "✓ Таблица vitrina_supplier_sales создана\n",
      "✓ Таблица vitrina_product_quality создана\n",
      "✓ Таблица top10_sold_products создана\n",
      "✓ Таблица category_revenue создана\n",
      "✓ Таблица top10_customers_by_spent создана\n",
      "✓ Таблица customer_country_distribution создана\n",
      "✓ Таблица top5_stores_by_revenue создана\n",
      "✓ Таблица top5_suppliers_by_revenue создана\n",
      "✓ Таблица product_quality_correlation создана\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def create_clickhouse_table(table_name, create_query):\n",
    "    \"\"\"Создание таблицы в ClickHouse через HTTP API\"\"\"\n",
    "    try:\n",
    "        url = \"http://clickhouse:8123/\"\n",
    "        response = requests.post(url, data=create_query)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"✓ Таблица {table_name} создана\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"✗ Ошибка создания {table_name}: {response.text}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ошибка при создании таблицы {table_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Создаем все таблицы\n",
    "print(\"Создание таблиц в ClickHouse...\")\n",
    "\n",
    "# 1. Витрина продаж по продуктам\n",
    "create_clickhouse_table(\"vitrina_product_sales\", \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS vitrina_product_sales (\n",
    "    product_id UInt32,\n",
    "    name String,\n",
    "    category String,\n",
    "    total_quantity UInt64,\n",
    "    total_revenue Decimal(15,2),\n",
    "    avg_rating Float32,\n",
    "    review_count UInt32\n",
    ") ENGINE = MergeTree()\n",
    "ORDER BY (category, product_id)\n",
    "\"\"\")\n",
    "\n",
    "# 2. Витрина продаж по клиентам\n",
    "create_clickhouse_table(\"vitrina_customer_sales\", \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS vitrina_customer_sales (\n",
    "    customer_id UInt32,\n",
    "    customer_name String,\n",
    "    country String,\n",
    "    total_spent Decimal(15,2),\n",
    "    order_count UInt32,\n",
    "    avg_check Decimal(15,2)\n",
    ") ENGINE = MergeTree()\n",
    "ORDER BY (country, customer_id)\n",
    "\"\"\")\n",
    "\n",
    "# 3. Витрина продаж по времени\n",
    "create_clickhouse_table(\"vitrina_time_sales\", \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS vitrina_time_sales (\n",
    "    year UInt16,\n",
    "    month UInt8,\n",
    "    total_revenue Decimal(15,2),\n",
    "    total_quantity UInt64,\n",
    "    order_count UInt32,\n",
    "    avg_check Decimal(15,2),\n",
    "    avg_order_size Float32\n",
    ") ENGINE = MergeTree()\n",
    "ORDER BY (year, month)\n",
    "\"\"\")\n",
    "\n",
    "# 4. Витрина продаж по магазинам\n",
    "create_clickhouse_table(\"vitrina_store_sales\", \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS vitrina_store_sales (\n",
    "    store_id UInt32,\n",
    "    store_name String,\n",
    "    store_city String,\n",
    "    store_country String,\n",
    "    total_revenue Decimal(15,2),\n",
    "    order_count UInt32,\n",
    "    avg_check Decimal(15,2)\n",
    ") ENGINE = MergeTree()\n",
    "ORDER BY (store_country, store_id)\n",
    "\"\"\")\n",
    "\n",
    "# 5. Витрина продаж по поставщикам\n",
    "create_clickhouse_table(\"vitrina_supplier_sales\", \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS vitrina_supplier_sales (\n",
    "    supplier_id UInt32,\n",
    "    supplier_name String,\n",
    "    supplier_country String,\n",
    "    total_revenue Decimal(15,2),\n",
    "    avg_price Decimal(15,2)\n",
    ") ENGINE = MergeTree()\n",
    "ORDER BY (supplier_country, supplier_id)\n",
    "\"\"\")\n",
    "\n",
    "# 6. Витрина качества продукции\n",
    "create_clickhouse_table(\"vitrina_product_quality\", \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS vitrina_product_quality (\n",
    "    product_id UInt32,\n",
    "    name String,\n",
    "    rating Float32,\n",
    "    review_count UInt32,\n",
    "    total_quantity UInt64,\n",
    "    total_revenue Decimal(15,2)\n",
    ") ENGINE = MergeTree()\n",
    "ORDER BY product_id\n",
    "\"\"\")\n",
    "\n",
    "# Дополнительные таблицы\n",
    "create_clickhouse_table(\"top10_sold_products\", \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS top10_sold_products (\n",
    "    product_id UInt32,\n",
    "    name String,\n",
    "    category String,\n",
    "    total_quantity UInt64,\n",
    "    total_revenue Decimal(15,2),\n",
    "    avg_rating Float32,\n",
    "    review_count UInt32\n",
    ") ENGINE = MergeTree()\n",
    "ORDER BY product_id\n",
    "\"\"\")\n",
    "\n",
    "create_clickhouse_table(\"category_revenue\", \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS category_revenue (\n",
    "    category String,\n",
    "    category_revenue Decimal(15,2)\n",
    ") ENGINE = MergeTree()\n",
    "ORDER BY category\n",
    "\"\"\")\n",
    "\n",
    "create_clickhouse_table(\"top10_customers_by_spent\", \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS top10_customers_by_spent (\n",
    "    customer_id UInt32,\n",
    "    customer_name String,\n",
    "    country String,\n",
    "    total_spent Decimal(15,2),\n",
    "    order_count UInt32,\n",
    "    avg_check Decimal(15,2)\n",
    ") ENGINE = MergeTree()\n",
    "ORDER BY customer_id\n",
    "\"\"\")\n",
    "\n",
    "create_clickhouse_table(\"customer_country_distribution\", \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS customer_country_distribution (\n",
    "    country String,\n",
    "    total_spent_by_country Decimal(15,2),\n",
    "    customer_count UInt32\n",
    ") ENGINE = MergeTree()\n",
    "ORDER BY country\n",
    "\"\"\")\n",
    "\n",
    "create_clickhouse_table(\"top5_stores_by_revenue\", \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS top5_stores_by_revenue (\n",
    "    store_id UInt32,\n",
    "    store_name String,\n",
    "    store_city String,\n",
    "    store_country String,\n",
    "    total_revenue Decimal(15,2),\n",
    "    order_count UInt32,\n",
    "    avg_check Decimal(15,2)\n",
    ") ENGINE = MergeTree()\n",
    "ORDER BY store_id\n",
    "\"\"\")\n",
    "\n",
    "create_clickhouse_table(\"top5_suppliers_by_revenue\", \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS top5_suppliers_by_revenue (\n",
    "    supplier_id UInt32,\n",
    "    supplier_name String,\n",
    "    supplier_country String,\n",
    "    total_revenue Decimal(15,2),\n",
    "    avg_price Decimal(15,2)\n",
    ") ENGINE = MergeTree()\n",
    "ORDER BY supplier_id\n",
    "\"\"\")\n",
    "\n",
    "create_clickhouse_table(\"product_quality_correlation\", \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS product_quality_correlation (\n",
    "    corr_rating_revenue Float64,\n",
    "    corr_rating_quantity Float64,\n",
    "    description String\n",
    ") ENGINE = MergeTree()\n",
    "ORDER BY description\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71d1678c-1ce3-469c-93b9-d05bc29ecb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создание витрины продаж по продуктам...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Данные записаны в таблицу vitrina_product_sales\n",
      "✓ Данные записаны в таблицу top10_sold_products\n",
      "✓ Данные записаны в таблицу category_revenue\n",
      "Создание витрины продаж по клиентам...\n",
      "✓ Данные записаны в таблицу vitrina_customer_sales\n",
      "✓ Данные записаны в таблицу top10_customers_by_spent\n",
      "✓ Данные записаны в таблицу customer_country_distribution\n",
      "Создание витрины продаж по времени...\n",
      "✓ Данные записаны в таблицу vitrina_time_sales\n",
      "Создание витрины продаж по магазинам...\n",
      "✗ Ошибка при записи в таблицу vitrina_store_sales: Column store_city not found in schema Some(StructType(StructField(store_id,DecimalType(20,0),false),StructField(store_name,StringType,false),StructField(city,StringType,false),StructField(country,StringType,false),StructField(total_revenue,DecimalType(15,2),false),StructField(order_count,DecimalType(20,0),false),StructField(avg_check,DecimalType(15,2),false))).\n",
      "✓ Данные записаны в таблицу top5_stores_by_revenue\n",
      "Создание витрины продаж по поставщикам...\n",
      "✗ Ошибка при записи в таблицу vitrina_supplier_sales: Column supplier_country not found in schema Some(StructType(StructField(supplier_id,DecimalType(20,0),false),StructField(supplier_name,StringType,false),StructField(country,StringType,false),StructField(total_revenue,DecimalType(15,2),false),StructField(avg_price,DecimalType(15,2),false))).\n",
      "✓ Данные записаны в таблицу top5_suppliers_by_revenue\n",
      "Создание витрины качества продукции...\n",
      "✓ Данные записаны в таблицу vitrina_product_quality\n",
      "✓ Данные записаны в таблицу product_quality_correlation\n",
      "============================================================\n",
      "ВСЕ ДАННЫЕ УСПЕШНО ЗАГРУЖЕНЫ В CLICKHOUSE!\n",
      "============================================================\n",
      "\n",
      "Проверка загруженных данных:\n",
      "✓ Таблица vitrina_product_sales: 6210 записей\n",
      "✓ Таблица vitrina_customer_sales: 10000 записей\n",
      "✓ Таблица vitrina_time_sales: 12 записей\n",
      "✓ Таблица vitrina_store_sales: 0 записей\n",
      "✓ Таблица vitrina_supplier_sales: 0 записей\n",
      "✓ Таблица vitrina_product_quality: 2947 записей\n",
      "✓ Таблица top10_sold_products: 10 записей\n",
      "✓ Таблица top10_customers_by_spent: 10 записей\n",
      "✓ Таблица top5_stores_by_revenue: 5 записей\n",
      "✓ Таблица top5_suppliers_by_revenue: 5 записей\n",
      "✓ Таблица product_quality_correlation: 1 записей\n"
     ]
    }
   ],
   "source": [
    "def write_to_clickhouse_existing(df, table_name):\n",
    "    \"\"\"Запись DataFrame в существующую таблицу ClickHouse\"\"\"\n",
    "    try:\n",
    "        df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", ch_jdbc_url) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"user\", ch_properties[\"user\"]) \\\n",
    "            .option(\"password\", ch_properties[\"password\"]) \\\n",
    "            .option(\"driver\", ch_properties[\"driver\"]) \\\n",
    "            .option(\"batchsize\", 100000) \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "        \n",
    "        print(f\"✓ Данные записаны в таблицу {table_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ошибка при записи в таблицу {table_name}: {e}\")\n",
    "\n",
    "# ===============================\n",
    "# 1. Витрина продаж по продуктам\n",
    "# ===============================\n",
    "print(\"Создание витрины продаж по продуктам...\")\n",
    "product_vitrina = fact.join(dim_product, fact.product_id == dim_product.product_id) \\\n",
    "    .groupBy(dim_product.product_id, dim_product.name, dim_product.category) \\\n",
    "    .agg(\n",
    "        sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        first(\"rating\").alias(\"avg_rating\"),\n",
    "        first(\"reviews\").alias(\"review_count\")\n",
    "    )\n",
    "\n",
    "write_to_clickhouse_existing(product_vitrina, \"vitrina_product_sales\")\n",
    "\n",
    "# Топ-10 самых продаваемых\n",
    "top10_products = product_vitrina.orderBy(desc(\"total_quantity\")).limit(10)\n",
    "write_to_clickhouse_existing(top10_products, \"top10_sold_products\")\n",
    "\n",
    "# Выручка по категориям\n",
    "category_revenue = product_vitrina.groupBy(\"category\") \\\n",
    "    .agg(sum(\"total_revenue\").alias(\"category_revenue\"))\n",
    "write_to_clickhouse_existing(category_revenue, \"category_revenue\")\n",
    "\n",
    "# ===============================\n",
    "# 2. Витрина продаж по клиентам\n",
    "# ===============================\n",
    "print(\"Создание витрины продаж по клиентам...\")\n",
    "customer_vitrina = fact.join(dim_customer, fact.customer_id == dim_customer.customer_id) \\\n",
    "    .groupBy(dim_customer.customer_id, dim_customer.first_name, dim_customer.last_name, dim_customer.country) \\\n",
    "    .agg(\n",
    "        sum(\"sale_total_price\").alias(\"total_spent\"),\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        avg(\"sale_total_price\").alias(\"avg_check\")\n",
    "    ) \\\n",
    "    .withColumn(\"customer_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\"))) \\\n",
    "    .select(\"customer_id\", \"customer_name\", \"country\", \"total_spent\", \"order_count\", \"avg_check\")\n",
    "\n",
    "write_to_clickhouse_existing(customer_vitrina, \"vitrina_customer_sales\")\n",
    "\n",
    "top10_customers = customer_vitrina.orderBy(desc(\"total_spent\")).limit(10)\n",
    "write_to_clickhouse_existing(top10_customers, \"top10_customers_by_spent\")\n",
    "\n",
    "customer_country_dist = customer_vitrina.groupBy(\"country\") \\\n",
    "    .agg(\n",
    "        sum(\"total_spent\").alias(\"total_spent_by_country\"),\n",
    "        count(\"*\").alias(\"customer_count\")\n",
    "    )\n",
    "write_to_clickhouse_existing(customer_country_dist, \"customer_country_distribution\")\n",
    "\n",
    "# ===============================\n",
    "# 3. Витрина продаж по времени\n",
    "# ===============================\n",
    "print(\"Создание витрины продаж по времени...\")\n",
    "time_vitrina = fact.join(dim_date, fact.date_id == dim_date.date_id) \\\n",
    "    .groupBy(dim_date.year, dim_date.month) \\\n",
    "    .agg(\n",
    "        sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        count(\"*\").alias(\"order_count\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_check\", col(\"total_revenue\") / col(\"order_count\")) \\\n",
    "    .withColumn(\"avg_order_size\", col(\"total_quantity\") / col(\"order_count\"))\n",
    "\n",
    "write_to_clickhouse_existing(time_vitrina, \"vitrina_time_sales\")\n",
    "\n",
    "# ===============================\n",
    "# 4. Витрина продаж по магазинам (с правильными колонками)\n",
    "# ===============================\n",
    "print(\"Создание витрины продаж по магазинам...\")\n",
    "store_vitrina = fact.join(dim_store, fact.store_id == dim_store.store_id) \\\n",
    "    .groupBy(dim_store.store_id, dim_store.store_name, dim_store.store_city, dim_store.store_country) \\\n",
    "    .agg(\n",
    "        sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        avg(\"sale_total_price\").alias(\"avg_check\")\n",
    "    )\n",
    "\n",
    "write_to_clickhouse_existing(store_vitrina, \"vitrina_store_sales\")\n",
    "\n",
    "top5_stores = store_vitrina.orderBy(desc(\"total_revenue\")).limit(5)\n",
    "write_to_clickhouse_existing(top5_stores, \"top5_stores_by_revenue\")\n",
    "\n",
    "# ===============================\n",
    "# 5. Витрина продаж по поставщикам (с правильными колонками)\n",
    "# ===============================\n",
    "print(\"Создание витрины продаж по поставщикам...\")\n",
    "supplier_vitrina = fact.join(dim_product, fact.product_id == dim_product.product_id) \\\n",
    "    .join(dim_supplier, fact.supplier_id == dim_supplier.supplier_id) \\\n",
    "    .groupBy(dim_supplier.supplier_id, dim_supplier.supplier_name, dim_supplier.supplier_country) \\\n",
    "    .agg(\n",
    "        sum(\"sale_total_price\").alias(\"total_revenue\"),\n",
    "        sum(col(\"price\") * col(\"sale_quantity\")).alias(\"weighted_price_sum\"),\n",
    "        sum(\"sale_quantity\").alias(\"total_quantity\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_price\", col(\"weighted_price_sum\") / col(\"total_quantity\")) \\\n",
    "    .select(\"supplier_id\", \"supplier_name\", \"supplier_country\", \"total_revenue\", \"avg_price\")\n",
    "\n",
    "write_to_clickhouse_existing(supplier_vitrina, \"vitrina_supplier_sales\")\n",
    "\n",
    "top5_suppliers = supplier_vitrina.orderBy(desc(\"total_revenue\")).limit(5)\n",
    "write_to_clickhouse_existing(top5_suppliers, \"top5_suppliers_by_revenue\")\n",
    "\n",
    "# ===============================\n",
    "# 6. Витрина качества продукции\n",
    "# ===============================\n",
    "print(\"Создание витрины качества продукции...\")\n",
    "quality_vitrina = fact.join(dim_product, fact.product_id == dim_product.product_id) \\\n",
    "    .groupBy(dim_product.product_id, dim_product.name) \\\n",
    "    .agg(\n",
    "        first(\"rating\").alias(\"rating\"),\n",
    "        first(\"reviews\").alias(\"review_count\"),\n",
    "        sum(\"sale_quantity\").alias(\"total_quantity\"),\n",
    "        sum(\"sale_total_price\").alias(\"total_revenue\")\n",
    "    )\n",
    "\n",
    "write_to_clickhouse_existing(quality_vitrina, \"vitrina_product_quality\")\n",
    "\n",
    "# Корреляция\n",
    "correlation = quality_vitrina.agg(\n",
    "    corr(\"rating\", \"total_revenue\").alias(\"corr_rating_revenue\"),\n",
    "    corr(\"rating\", \"total_quantity\").alias(\"corr_rating_quantity\")\n",
    ").withColumn(\"description\", lit(\"Correlation between rating and sales\"))\n",
    "\n",
    "write_to_clickhouse_existing(correlation, \"product_quality_correlation\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ВСЕ ДАННЫЕ УСПЕШНО ЗАГРУЖЕНЫ В CLICKHOUSE!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Проверка данных\n",
    "def check_table_count(table_name):\n",
    "    \"\"\"Проверка количества записей в таблице\"\"\"\n",
    "    try:\n",
    "        count_df = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", ch_jdbc_url) \\\n",
    "            .option(\"dbtable\", f\"(SELECT count(*) as cnt FROM {table_name}) as t\") \\\n",
    "            .option(\"user\", ch_properties[\"user\"]) \\\n",
    "            .option(\"password\", ch_properties[\"password\"]) \\\n",
    "            .option(\"driver\", ch_properties[\"driver\"]) \\\n",
    "            .load()\n",
    "        count = count_df.first()[\"cnt\"]\n",
    "        print(f\"✓ Таблица {table_name}: {count} записей\")\n",
    "        return count\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ошибка при проверке таблицы {table_name}: {e}\")\n",
    "        return 0\n",
    "\n",
    "print(\"\\nПроверка загруженных данных:\")\n",
    "tables_to_check = [\n",
    "    \"vitrina_product_sales\", \"vitrina_customer_sales\", \"vitrina_time_sales\",\n",
    "    \"vitrina_store_sales\", \"vitrina_supplier_sales\", \"vitrina_product_quality\",\n",
    "    \"top10_sold_products\", \"top10_customers_by_spent\", \"top5_stores_by_revenue\",\n",
    "    \"top5_suppliers_by_revenue\", \"product_quality_correlation\"\n",
    "]\n",
    "\n",
    "for table in tables_to_check:\n",
    "    check_table_count(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c3fa83-b36d-4ff5-a95c-1dddd3133a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
